{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from baseline import Baseline\n",
    "from system_t import System_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AdultCensus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'country', 'target']\n"
     ]
    }
   ],
   "source": [
    "def adult_custom_preprocessing(df):\n",
    "    def group_race(x):\n",
    "        if x == \"White\":\n",
    "            return \"White\"\n",
    "        elif x == \"Black\":\n",
    "            return \"Black\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    def group_country(x):\n",
    "        if x != \"United-States\":\n",
    "            return \"Non-US\"\n",
    "        else:\n",
    "            return \"US\"\n",
    "        \n",
    "    \n",
    "    df['race'] = df['race'].apply(lambda x: group_race(x))\n",
    "    df['country'] = df['country'].apply(lambda x: group_country(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def transform(train_data, test_data):\n",
    "    drop_columns = ['fnlwgt']\n",
    "\n",
    "    train_data.drop(drop_columns, axis=1, inplace=True)\n",
    "    test_data.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "    train_data = adult_custom_preprocessing(train_data)\n",
    "    test_data = adult_custom_preprocessing(test_data)\n",
    "\n",
    "    x_train, y_train = data_preprocessing(train_data)\n",
    "    x_test, y_test = data_preprocessing(test_data, False)\n",
    "\n",
    "    missing_cols = set(x_train.columns) - set(x_test.columns)\n",
    "    for column in missing_cols:\n",
    "        x_test[column] = 0\n",
    "    x_test = x_test[x_train.columns]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def data_preprocessing(data, train=True):\n",
    "    data_copy = data.copy()\n",
    "    data_copy[\"target\"] = data_copy[\"target\"].apply(lambda x:0 if (x=='<=50K' or x=='<=50K.') else 1)\n",
    "    x_data = data_copy.drop('target', axis =1)\n",
    "    y_data = data_copy[\"target\"]        \n",
    "\n",
    "    num_data = x_data.select_dtypes(exclude='object')\n",
    "    cat_data = x_data.select_dtypes(include='object')\n",
    "\n",
    "    if train:\n",
    "        num_data = pd.DataFrame(scalar.fit_transform(num_data), columns=num_data.columns)\n",
    "    else:\n",
    "        num_data = pd.DataFrame(scalar.transform(num_data), columns=num_data.columns)\n",
    "    cat_data = pd.get_dummies(cat_data)\n",
    "\n",
    "    x_data = pd.concat([num_data, cat_data], axis=1)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "schema = open(\"/home/kihyun/system/adult/data/schema\").read().splitlines()\n",
    "print(schema)\n",
    "train_data = pd.read_csv(\"/home/kihyun/system/adult/data/train.data\", sep=r'[,\\t ]+', header=None, names=schema, na_values='?')\n",
    "test_data = pd.read_csv(\"/home/kihyun/system/adult/data/test.data\", sep=r'[,\\t ]+', header=None, names=schema, na_values='?')    \n",
    "    \n",
    "train_data = train_data.dropna().reset_index(drop=True)\n",
    "test_data = test_data.dropna().reset_index(drop=True)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "x_train, y_train, x_test, y_test = transform(train_data, test_data)\n",
    "\n",
    "x_data = pd.concat([x_train, x_test])\n",
    "y_data = pd.concat([y_train, y_test]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic setting: slices have the same amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of slices : 4, 2\n"
     ]
    }
   ],
   "source": [
    "def shuffle(data, label):\n",
    "    shuffle = np.arange(len(data))\n",
    "    np.random.shuffle(shuffle)\n",
    "    data = data[shuffle]\n",
    "    label = label[shuffle]\n",
    "    return data, label\n",
    "\n",
    "initial_data_array = []\n",
    "val_data_dict = []\n",
    "add_data_dict = []\n",
    "    \n",
    "val_data_num = 500\n",
    "\n",
    "feature_index = 0\n",
    "gender_list = [\"sex_Female\", \"sex_Male\"]\n",
    "race_list = [\"race_White\", \"race_Black\"]\n",
    "\n",
    "for gender in gender_list:\n",
    "    for race in race_list:\n",
    "        data_num = 150\n",
    "        initial_data_array.append(data_num)\n",
    "        temp_x, temp_y = x_data[(x_data[gender] == 1) & (x_data[race] == 1)], y_data[(x_data[gender] == 1) & (x_data[race] == 1)]\n",
    "\n",
    "        val_data_dict.append((temp_x[data_num:data_num+val_data_num].to_numpy(), tf.keras.utils.to_categorical(temp_y[data_num:data_num+val_data_num])))\n",
    "        add_data_dict.append((temp_x[data_num+val_data_num:].to_numpy(), tf.keras.utils.to_categorical(temp_y[data_num+val_data_num:])))\n",
    "        if feature_index == 0:\n",
    "            train_data = temp_x[:data_num]\n",
    "            train_label = temp_y[:data_num]\n",
    "\n",
    "            val_data = temp_x[data_num:data_num+val_data_num]\n",
    "            val_label = temp_y[data_num:data_num+val_data_num]\n",
    "        else:\n",
    "            train_data = pd.concat([train_data, temp_x[:data_num]])\n",
    "            train_label = pd.concat([train_label, temp_y[:data_num]]) \n",
    "            val_data = pd.concat((val_data, temp_x[data_num:data_num+val_data_num]))\n",
    "            val_label = pd.concat((val_label, temp_y[data_num:data_num+val_data_num])) \n",
    "        feature_index += 1\n",
    "        \n",
    "num_label = len(np.unique(train_label))\n",
    "num_class = feature_index \n",
    "print(\"Number of slices : %d, %d\" % (num_class, num_label))\n",
    "\n",
    "train_data = train_data.to_numpy()\n",
    "train_label = tf.keras.utils.to_categorical(train_label)\n",
    "\n",
    "val_data = val_data.to_numpy()\n",
    "val_label = tf.keras.utils.to_categorical(val_label)\n",
    "\n",
    "train_data, train_label = shuffle(train_data, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice: White-Female, Initial size: 150\n",
      "Slice: Black-Female, Initial size: 150\n",
      "Slice: White-Male, Initial size: 150\n",
      "Slice: Black-Male, Initial size: 150\n"
     ]
    }
   ],
   "source": [
    "slice_desc = []\n",
    "a = [\"White-Female\", \"Black-Female\", \"White-Male\", \"Black-Male\"]\n",
    "\n",
    "for i in range(num_class):\n",
    "    slice_desc.append('Slice: %s' % (a[i]))\n",
    "    print('Slice: %s, Initial size: %s' % (a[i], initial_data_array[i]))\n",
    "    \n",
    "feature_list = np.array(x_data.columns)\n",
    "slice_index = []\n",
    "for gender in gender_list:\n",
    "    for race in race_list:\n",
    "        ind1 = np.where(feature_list == gender)[0][0]\n",
    "        ind2 = np.where(feature_list == race)[0][0]\n",
    "        slice_index.append([ind1, ind2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original ( with no data acquisition ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Uniform, Budget: 0\n",
      "======= Collect Data =======\n",
      "[0 0 0 0]\n",
      "======= Performance =======\n",
      "Loss: 0.26328 (0.00201), Average EER: 0.10336 (0.00163), Max EER: 0.16613 (0.00386)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost_func = [1] * num_class\n",
    "lr = 0.001\n",
    "\n",
    "ori = Baseline((train_data, train_label), (val_data, val_label), val_data_dict, \n",
    "                initial_data_array, num_class, num_label, slice_index, add_data_dict, method='Uniform')\n",
    "ori.performance(budget=0, cost_func=cost_func, num_iter=10, batch_size=32, lr=lr, epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System T Demo on AdultCensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use 300 budget, lambda=0.1, \"Moderate\" strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Collect Data =======\n",
      "[  0  75 131  94]\n",
      "Total Cost: 300, Remaining Budget: 0\n",
      "\n",
      "======= Performance =======\n",
      "[  0.  75. 131.  94.]\n",
      "Number of iteration: 1\n",
      "Strategy: Moderate, C: 0.1, Budget: 300\n",
      "Loss: 0.25341 (0.00100), Average EER: 0.09448 (0.00052), Max EER: 0.14806 (0.00081)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "budget = 300\n",
    "method = 'Moderate'\n",
    "\n",
    "st = System_T((train_data, train_label), (val_data, val_label), val_data_dict, initial_data_array, num_class, num_label, slice_index, add_data_dict)\n",
    "st.selective_collect(budget=budget, k=10, batch_size=32, lr = lr, epochs=2000, cost_func=cost_func, \n",
    "                 Lambda=0.1, num_iter=5, slice_desc=slice_desc, strategy=method, show_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Uniform ( = Water filling )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a basic setting, Uniform method is equivalent to Water filling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Uniform, Budget: 300\n",
      "======= Collect Data =======\n",
      "[75 75 75 75]\n",
      "======= Performance =======\n",
      "Loss: 0.25707 (0.00205), Average EER: 0.09862 (0.00086), Max EER: 0.15513 (0.00164)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "budget = 300\n",
    "uni = Baseline((train_data, train_label), (val_data, val_label), val_data_dict, \n",
    "                initial_data_array, num_class, num_label, slice_index, add_data_dict, method='Uniform')\n",
    "uni.performance(budget=budget, cost_func=cost_func, num_iter=10, batch_size=32, lr=lr, epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <div align=\"center\"> -->\n",
    "    \n",
    "|<font size=\"5\">Method</font>  | <font size=\"5\"> Loss </font>| <font size=\"5\"> Avg.EER </font>| \n",
    "|:---------------------:|:---------------------:|:---------------------:| \n",
    "|<font size=5> Original </font> | <font size=5> 0.26328  (&pm; 0.00101) </font> | <font size=5> 0.10336 (&pm; 0.00081)</font>|\n",
    "|<font size=5> Uniform </font> | <font size=5> 0.25707  (&pm; 0.00103) </font> | <font size=5> 0.09862 (&pm; 0.00043)</font>|\n",
    "|<font size=5> Water filling </font> | <font size=5> 0.25707  (&pm; 0.00103) </font> | <font size=5> 0.09862 (&pm; 0.00043)</font>|\n",
    "|<font size=5> Moderate (ours) </font> | <font size=5> 0.25341  (&pm; 0.00050) </font> | <font size=5> 0.09448 (&pm; 0.00026)</font>|\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
